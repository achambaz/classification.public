---
title: "Régression par &laquo;exteme gradient boosting&raquo;"
author: "Antoine Chambaz"
date: "23/10/2017"
encoding: "UTF-8"
output:
  github_document:
  toc: yes
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(
    warnings = FALSE,
	fig.width = 12, 
	fig.height = 4, 
	fig.path = 'img/'
)
```


## Les notions

* Algorithme de régression par &laquo;extreme gradient boosting&raquo;.

* Ensembles d'apprentissage et de validation

* &laquo;Machine-learning pipelines&raquo;

* Principe de validation croisée

* Reproductibilité


## Fichier source

Afin   d'extraire   les    portions   de   code   `R`    du   fichier   source
[`regression.xgboost.Rmd`](https://github.com/achambaz/laviemodedemploi/blob/master/regression.xgboost/regression.xgboost.Rmd),
il       suffit       d'exécuter        dans       `R`       la       commande
`knitr::purl("regression.xgboost.Rmd")`.

## Préparation de la session `R`

```{r session:one, eval = FALSE}
pkgs <- c("xgboost", "tidyverse", "devtools")
for (pkg in pkgs) {
  if (! (pkg %in% rownames(installed.packages()))) {
    install.packages(pkg)
  }
}
pkg <- "pipelearner"
if (! (pkg %in% rownames(installed.packages()))) {
  devtools::install_github("drsimonj/pipelearner")
}
```

```{r session:two}
suppressMessages(library(tidyverse))
suppressMessages(library(lazyeval))
```

## Une introduction à la régression par &laquo;extreme gradient boosting&raquo;

```{r preliminary}
set.seed(54321)
```

* Mises  à disposition par  Météo France, ces  données sont extraites  du site
[wikistat](https://github.com/wikistat). Nous souhaitons  apprendre à prédire,
à partir des données du jour, la concentration dioxyde d'azote le lendemain.

```{r load_ozone}
one_hot <- function(df) {
  as_tibble(stats::model.matrix(~.+0, data = df))
}

file <- file.path("http://www.math.univ-toulouse.fr/~besse/Wikistat/data", "depSeuil.dat")
ozone <- read_csv(file, col_names = TRUE) %>% one_hot %>% mutate_all(as.numeric)

## JOUR: jour férié (1) ou pas (0)
## O3obs: concentration d'ozone effectivement observée le lendemain à 17h locales (correspond souvent au maximum de pollution observée)
## MOCAGE: prévision de cette pollution obtenue par un modèle déterministe de mécanique des fluides
## TEMPE: température prévue par Météo France pour le lendemain 17h
## RMH2O: rapport d'humidité
## NO2: concentration en dioxyde d'azote
## NO: concentration en monoxyde d'azote
## STATION: lieu de l'observation (Aix-en-Provence, Rambouillet, Munchhausen, Cadarache et Plan de Cuques)
## VentMOD: force du vent
## VentANG: orientation du vent

head(ozone)
```

<!-- [lien intéressant](http://www.win-vector.com/blog/2017/04/encoding-categorical-variables-one-hot-and-beyond/)-->

A quoi  la fonction `one_hot` sert-elle?  Noter que nous avons  dû transformer
toutes les informations au format `numeric`.  

*Remarque.* La variable  `STATION` se prêterait volontiers  à un recodage
plus riche&hellip;

* Préparation d'un ensemble d'apprentissage et d'un ensemble de validation.

```{r splitting}
m <- nrow(ozone)
val <- sample(1:m, size = round(m/3), replace = FALSE, prob = rep(1/m, m)) 
ozone.train <- ozone[-val, ]
ozone.test <- ozone[val, ]
```

* Régression `xgboost`

Le  nom   `xgboost`  est  inspiré  de   l'expression  &laquo;extreme  gradient
boosting&raquo; Cet  algorithme d'apprentissage est aujourd'hui  très apprécié
dans  la  communauté du  &laquo;machine  learning&raquo;.  Vous trouverez  une
brève  introduction  aux  principes  sur lesquels  cet  algorithme  est  fondé
[ici](http://xgboost.readthedocs.io/en/latest/model.html). 

Sans finasser&hellip;

```{r xgboost:one}
suppressMessages(library(xgboost))

get.rmse <- function(fit, newdata, target_var) {
  ## Convert 'newdata' object to data.frame
  newdata <- as.data.frame(newdata)
  ## Get feature matrix and labels
  X <- newdata %>%
    select(-matches(target_var)) %>% 
    as.matrix()
  Y <- newdata[[target_var]]
  ## Compute and return 'rmse'
  sqrt( mean((Y - predict(fit, X))^2) )
}


ozone.train.X <- select(ozone.train, -NO2) %>% as.matrix
ozone.train.Y <- ozone.train$NO2

nrounds <- 50
fit.xgboost.one <- xgboost(data = ozone.train.X, label = ozone.train.Y,
                           nrounds = nrounds, objective = "reg:linear", print_every = 10)

rmse.test.one <- get.rmse(fit.xgboost.one, ozone.test, "NO2")
rmse.test.one
```

Aurions-nous gagné à  jouer sur les paramètres? Ici, nous  passons la variable
`eta` de sa valeur par défaut, `0.3`, à `0.1`.

```{r xgbosst:two}
params <- list(booster = "gbtree", objective = "reg:linear", eta = 0.1, gamma = 0,
               max_depth = 6, min_child_weight = 1, subsample = 1, colsample_bytree = 1)

fit.xgboost.two <- xgboost(data = ozone.train.X, label = ozone.train.Y,
                           nrounds = nrounds, params = params, print_every = 10)

rmse.test.two <- get.rmse(fit.xgboost.two, ozone.test, "NO2")
rmse.test.two
```

Ou bien, aurions-nous gagné à stopper les itérations plus tôt?

```{r xgboost:three}
params <- list(booster = "gbtree", objective = "reg:linear", eta = 0.3, gamma = 0,
               max_depth = 6, min_child_weight = 1, subsample = 1, colsample_bytree = 1)

fit.xgboost.cv <- xgb.cv(data = ozone.train.X, label = ozone.train.Y,
                         nrounds = nrounds, nfold = 5, params = params, print_every = 10,
                         showsd = TRUE, early.stopping.rounds = 20, maximize = FALSE)
best.xgboost.count <- which.min(fit.xgboost.cv$evaluation_log$test_rmse_mean)
fit.xgboost.three <- xgboost(data = ozone.train.X, label = ozone.train.Y,
                             nrounds = best.xgboost.count, params = params, print_every = 10)

rmse.test.three <- get.rmse(fit.xgboost.three, ozone.test, "NO2")
best.xgboost.count
rmse.test.three
```

## Mise en place d'une &laquo;ML pipeline&raquo;

Cette             section             s'inspire             de             [ce
billet](https://drsimonj.svbtle.com/with-our-powers-combined-xgboost-and-pipelearner),
&amp; il faudrait trouver une jolie expression française.


Nous   commençons  par   définir   les  fonctions   `r.square`  (inspirée   de
`modelr::rsquare`,  mais  capable de  s'adapter  à  la présence  de  nouvelles
données) et  `pl.xgboost` (une  version de `xgboost`  se prêtant  au &laquo;ML
pipelining&raquo;).

```{r ML:pipe:one}
get.params <- function(ll) {
  sprintf("nrounds=%i,eta=%.1f,gamma=%.1f,max_depth=%i", ll$nrounds, ll$eta, ll$gamma, ll$max_depth)
}
get.rsquare <- function(fit, newdata, target_var) {
  ## Convert 'newdata' object to data.frame
  newdata <- as.data.frame(newdata)
  ## Get feature matrix and labels
  actual <- newdata[[target_var]]
  X <- newdata %>%
    select(-matches(target_var)) %>% 
    as.matrix()
  residuals <- predict(fit, X) - actual
  ## Compute and return 'rsquare'
  1 - (var(residuals, na.rm = TRUE) / var(actual, na.rm = TRUE))
}
pl.xgboost <- function(data, formula, ...) {
  data <- as.data.frame(data)

  X_names <- as.character(f_rhs(formula))
  y_name  <- as.character(f_lhs(formula))

  if (X_names == '.') {
    X_names <- names(data)[names(data) != y_name]
  }

  X <- data.matrix(data[, X_names])
  y <- data[[y_name]]

  xgboost(data = X, label = y, ...)
}
```

*  Nous  voilà  enfin  prêts  à  procéder  à  l'évaluation  d'une  variété  de
paramétrisations de `xgboost` par &laquo;ML pipelining&raquo;

```{r ML:pipe:two}
suppressMessages(library(pipelearner))

pl <- pipelearner(ozone.train, pl.xgboost, NO2 ~ .,
                  nrounds = seq(10, 60, 10),
                  eta = seq(0.1, 0.5, 0.1),
                  gamma = seq(0.0, 0.5, 0.1),
                  max_depth = seq.int(2, 7, 1),
                  verbose = 0) %>%
  learn_cvpairs(crossv_kfold, k = 5)

fits.xgboost <- pl %>% learn()

results.xgboost <- fits.xgboost %>% 
  mutate(
    ## Get hyperparameters
    # nrounds = map_dbl(params, "nrounds"),
    # eta = map_dbl(params, "eta"),
    # gamma = map_dbl(params, "gamma"),
    # max_depth = map_dbl(params, "max_depth"),
    negm = map_chr(params, get.params),
    ## Get rmse
    rmse.train = pmap_dbl(list(fit, train, target), get.rmse),
    rmse.test  = pmap_dbl(list(fit, test,  target), get.rmse),
    ## Get r-square
    rsquare.train = pmap_dbl(list(fit, train, target), get.rsquare),
    rsquare.test  = pmap_dbl(list(fit, test, target), get.rsquare)
  ) %>%
  group_by(negm) %>%
  summarise(
    mrmse.train = mean(rmse.train),
    mrmse.test = mean(rmse.test),
    mrsquare.train = mean(rsquare.train),
    mrsquare.test = mean(rsquare.test),    
  ) %>%
  ## Order rows
  arrange(desc(mrmse.test))

tail(results.xgboost)
```

*    Pour  conclure, visualisons  les différentes  mesures de  perfomance.  En
  abscisse,  celles  obtenues  sur l'ensemble  d'apprentissage;  en  ordonnée,
  celles  obtenues sur  l'ensemble de  validation  (ordonnée), au  sein de  la
  procédure  de   validation  croisée   &mdash;  seules  ces   dernières  sont
  pertinentes. 

```{r multiplot, echo = FALSE}
## Multiple plot function
##
## ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
## - cols:   Number of columns in layout
## - layout: A matrix specifying the layout. If present, 'cols' is ignored.
##
## If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
## then plot 1 will go in the upper left, 2 will go in the upper right, and
## 3 will go all the way across the bottom.
##
## see http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/

multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  ## Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  ## If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    ## Make the panel
    ## ncol: Number of columns of plots
    ## nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    ## Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    ## Make each plot, in the correct location
    for (i in 1:numPlots) {
      ## Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```

```{r ML:pipe:three}
fig.one <- results.xgboost %>%
  ggplot(aes(mrsquare.train, mrsquare.test, color = negm)) +
  guides(color = FALSE) +
  geom_point(size = 2) +
  geom_abline(intercept = 0, slope = 1, color = "grey")

fig.two <- results.xgboost %>%
  ggplot(aes(mrmse.train, mrmse.test, color = negm)) +
  guides(color = FALSE) +
  geom_point(size = 2) +
  geom_abline(intercept = 0, slope = 1, color = "grey")

multiplot(fig.one, fig.two, cols = 2)
```

D'après  le  résultat de  la  commande  ``tail(results.xgboost)`, le  meilleur
paramétrage parmi ceux  essayés (identifié selon le  risque quadratique validé
croisé) correspond à `r tail(results.xgboost, 1)$negm`. En conclusion:

```{r ML:pipe:four}
eval(parse(text = paste("params <- list(
booster = \"gbtree\", objective = \"reg:linear\",
min_child_weight = 1, subsample = 1, colsample_bytree = 1,",
tail(results.xgboost, 1)$negm,
")")))

fit.xgboost.best <- xgboost(data = ozone.train.X, label = ozone.train.Y,
                            nrounds = nrounds, params = params, print_every = 10)

rmse.test.best <- get.rmse(fit.xgboost.best, ozone.test, "NO2")
rmse.test.best
```

[Retour à la table des matières](https://github.com/achambaz/laviemodedemploi#liens)
